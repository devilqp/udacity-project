# Data Engineer Nanodegree - Project 3: Data Warehouse on AWS

- [Data Engineer Nanodegree - Project 3: Data Warehouse on AWS](#data-engineer-nanodegree---project-3-data-warehouse-on-aws)
  - [Overview](#overview)
  - [Dataset](#dataset)
  - [Schema](#schema)
  - [Project Files](#project-files)
  - [How to run](#how-to-run)
    - [1. Config file](#1-config-file)
    - [2. Requirements](#2-requirements)
    - [3. Create tables](#3-create-tables)
    - [4. ETL](#4-etl)
    - [5. Test and inspect](#5-test-and-inspect)
  - [Purpose of the database](#purpose-of-the-database)

## Overview
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

## Dataset

The project is based on two dataset:
- **Song dataset**: it is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

    song_data/A/B/C/TRABCEI128F424C983.json
    song_data/A/A/B/TRAABJL12903CDCF1A.json

    And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

    ```json
    {
        "num_songs": 1,
        "artist_id": "ARJIE2Y1187B994AB7",
        "artist_latitude": null,
        "artist_longitude": null,
        "artist_location": "",
        "artist_name": "Line Renaud",
        "song_id": "SOUPIRU12A6D4FA1E1",
        "title": "Der Kleine Dompfaff",
        "duration": 152.92036,
        "year": 0
    }
    ```

- **Log Dataset**: it consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

    The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

    ```
    log_data/2018/11/2018-11-12-events.json
    log_data/2018/11/2018-11-13-events.json
    ```

## Schema
In order to analyze and query the data, Sparkify wants to create an OLAP database with PostgreSQL. After the etl process, the star schema resulting in the database will be the following:

**Staging tables**
The staging table have the same schema of the song files and the log files.

**Songplays table** (Fact table)
| Column name | Type      | Properties        |
| ----------- | --------- | ----------------- |
| songplay_id | BIGINT    | PK, AUTOINCREMENT |
| start_time  | timestamp |                   |
| user_id     | int       |                   |
| level       | varchar   |                   |
| song_id     | varchar   |                   |
| artist_id   | varchar   |                   |
| session_id  | int       |                   |
| location    | varchar   |                   |
| user_agent  | varchar   |                   |

**Songs**
| Column name | Type    | Properties |
| ----------- | ------- | ---------- |
| song_id     | varchar | PK         |
| title       | varchar |            |
| artist_id   | varchar |            |
| year        | int     |            |
| duration    | numeric |            |

**Artists**
| Column name | Type    | Properties |
| ----------- | ------- | ---------- |
| artist_id   | varchar | PK         |
| name        | varchar |            |
| location    | varchar |            |
| latitude    | numeric |            |
| longitude   | numeric |            |

**Users**
| Column name | Type    | Properties |
| ----------- | ------- | ---------- |
| user_id     | varchar | PK         |
| firsname    | varchar |            |
| lastname    | varchar |            |
| gender      | char    |            |
| level       | varchar |            |

**Time**
| Column name | Type      | Properties |
| ----------- | --------- | ---------- |
| start_time  | timestamp | PK         |
| hour        | int       |            |
| day         | int       |            |
| week        | int       |            |
| month       | int       |            |
| year        | int       |            |
| weekday     | int       |            |

## Project Files

```sql_queries.py``` -> contains sql queries for dropping and creating fact and dimension tables. Also, contains insertion query template

```create_tables.py``` -> contains code for setting up database. Running this file connecting to Redshift cluster and also creates the fact and dimension tables

```etl.py``` -> connecting to Redshift cluster, load staging tables and insert data

```dwh.txt``` -> template configuration file

```requirements.txt``` -> list of dependency libraries

## How to run

### 1. Config file
Rename the `dwh.txt` file in `dwh.cfg` and complete it with the correct informations.
### 2. Requirements
Be sure to have all requirements satisfied executing:
```
pip install -r requirements.txt
```
### 3. Create tables
Then, create tables using the `create_tables.py` Python script.
### 4. ETL
Now, you are ready to run the etl process executing the `etl.py` script.
### 5. Test and inspect
Go to your Redshift page and start querying the database.

## Purpose of the database

Sparkify wants to create this database to analyze and get insight from the data collected by their app. In fact, the app collects a lot of user actions, but the proble is: how to process all these data and how to get infromations from this logs? Creating an OLAP db like this, Sparkify can query the database, aggregate data and understand what songs users are listening.

Looking at the database, some possible and useful queries that Sparkify's team can execute are:
- Trending songs in a certain location
    ```SQL
    SELECT s.song_id, count(*) as plays
    FROM songplays AS sp JOIN songs AS s ON sp.song_id = s.song_id
    WHERE location = 'Chicago-Naperville-Elgin, IL-IN-WI'
    GROUP BY s.song_id
    ORDER BY plays DESC
    ```
- Trending artists in a certain location (similar to the previous query)
- How many songs are listened per session by user with free and paid level
    ```sql
    SELECT level, avg(songs)
    FROM (
            SELECT session_id, level, count(*) AS songs
            FROM songplays
            GROUP BY session_id, level
        ) AS t1
    GROUP BY level
    ```
- Top 20 songs for specific a year
  ```sql
  SELECT sp.song_id, title, count(*) AS plays
  FROM songplays AS sp JOIN songs AS s ON sp.song_id = s.song_id
    JOIN time t ON sp.start_time = t.start_time
  WHERE t.year = 2018
  GROUP BY sp.song_id, title
  ORDER BY plays DESC
  LIMIT 20

  ```

A lot of other queries can be executed to analyze the populatiry of songs and artists, also using geographic information, or analyze user behavior at a specific period of the day/month/week.